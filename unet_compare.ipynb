{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psu-rdmap/unet-compare/blob/main/unet_compare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogB11jwrz2_t"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "1. Click *Open in Colab* at the top\n",
        "\n",
        "2. Save a copy of this notebook via *File* > *Save a copy in Drive*\n",
        "\n",
        "3. Go to [Google Drive](https://drive.google.com) and create a new folder called `data/`.\n",
        "\n",
        "4. Create your dataset with the structure shown in the *Training & Inference* section of GitHub repository and place it in `data/`.\n",
        "\n",
        "5. Navigate to the notebook copy and select the drop down menu next to *Connect* in the top-right. Select *Change runtime type*, choose an available GPU, and select *Save*. Connect to a runtime\n",
        "\n",
        "6. Run all cells in *Source*. Note, you will have to authorize mounting to Google Drive\n",
        "\n",
        "7. Open the *Training/Inference* section, set the configs using the information [here](https://github.com/psu-rdmap/unet-compare/tree/main/configs), and run all cells in this section\n",
        "\n",
        "8. Operation will begin and a results folder will be created in your Google Drive containing files mentioned in the *Training and Inference* section of the GitHub repository \n",
        "\n",
        "9. To run it again, repeat 5-8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3y6pXwAzpfw"
      },
      "source": [
        "# Source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edA7kjp2pJbM"
      },
      "source": [
        "## Prepare Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D7TBTz_rPPT",
        "outputId": "c332283b-cb59-4d4b-96d1-0eb8d05cf039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2VTZfcqKdT-",
        "outputId": "a74babb2-046a-4d9e-ee0f-d48fe9c1c406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.8.0\n",
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from pydantic import BaseModel, PositiveInt, PositiveFloat, NonNegativeFloat, ConfigDict, Field, field_validator, model_validator\n",
        "from typing import Literal, List, Optional, Tuple\n",
        "from pathlib import Path\n",
        "from warnings import warn\n",
        "import numpy as np\n",
        "from natsort import os_sorted\n",
        "from datetime import datetime\n",
        "import cv2 as cv\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Conv2DTranspose, MaxPooling2D, Concatenate\n",
        "from keras.regularizers import l2\n",
        "import shutil, random, re, gc, keras, math, json\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB7\n",
        "from keras.preprocessing.image import save_img\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.api.optimizers import Adam\n",
        "from keras.api.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from keras.api.saving import load_model\n",
        "from keras import backend as K\n",
        "\n",
        "random.seed(229)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "tf.random.set_seed(3051)\n",
        "\n",
        "print(keras.__version__)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYtRIYVaKgLl"
      },
      "source": [
        "## Input Validator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iIcDj1qKjzt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aiden Ochoa, 4/2025, RDMAP PSU Research Group\n",
        "This module validates the user input configs file and modifies it as necessary\n",
        "\"\"\"\n",
        "\n",
        "ROOT_DIR = Path('/content') / 'drive' / 'MyDrive'\n",
        "\n",
        "\n",
        "class General(BaseModel):\n",
        "    \"\"\"Most general configs that apply to all possible use cases. Only basic validation is done here.\"\"\"\n",
        "\n",
        "    root_dir: Path = ROOT_DIR\n",
        "    operation_mode: Literal['train', 'inference'] = Field(\n",
        "        default='train',\n",
        "        description=\"General parameter that defines whether a model will be trained, or if a model will be applied for inference\"\n",
        "    )\n",
        "    dataset_name: str = Field(\n",
        "        min_length=2,\n",
        "        description=\"Dataset subdirectory prefix corresponding to unet-compare/data/<dataset_name>/\"\n",
        "    )\n",
        "    results_dir: Optional[str] = Field(\n",
        "        default=None,\n",
        "        description=\"Path for results directory relative to /path/to/unet-compare/. Give it `null` or ignore it to use default naming scheme\"\n",
        "    )    \n",
        "    model_config = ConfigDict(\n",
        "        extra='allow',\n",
        "    )\n",
        "\n",
        "\n",
        "class Train(BaseModel):\n",
        "    root_dir : Path\n",
        "    operation_mode: str\n",
        "    dataset_name: str\n",
        "    results_dir: Optional[str | Path]\n",
        "    input_shape: Tuple[int, int, int] = None\n",
        "    encoder_name: Literal['UNet', 'EfficientNetB7'] = Field(\n",
        "        default='UNet',\n",
        "        description=\"Type of model architecture forming the encoder section of U-Net\"\n",
        "    )\n",
        "    decoder_name: Literal['UNet', 'UNet++'] = Field(\n",
        "        default='UNet',\n",
        "        description=\"Type of model architecture forming the decoder section of U-Net\"\n",
        "    )\n",
        "    encoder_filters: Optional[List[PositiveInt]] = Field(\n",
        "        default=None,\n",
        "        min_length=5, \n",
        "        max_length=5,\n",
        "        description=\"Number of filters to learn for each convolution. Should be `null` or ignored when `encoder_name` is `EfficientNetB7`\"\n",
        "    )\n",
        "    decoder_filters: List[PositiveInt] = Field(\n",
        "        default = [512, 256, 128, 64, 32],\n",
        "        description=\"Number of filters to learn at each resolution level. The final item is only used when `encoder_name` is `EfficientNetB7`\"\n",
        "    )\n",
        "    backbone_weights: Optional[Literal['random', 'imagenet']] = Field(\n",
        "        default=None,\n",
        "        description=\"Weights to be loaded when using a pretrained backbone. This should be ignored when `encoder_name` is `UNet`\"\n",
        "    )\n",
        "    backbone_finetuning: Optional[bool | List[PositiveInt]] = Field(\n",
        "        default=None,\n",
        "        description=\"Controls the finetuning of a pretrained backbone. \" \\\n",
        "        \"This should be ignored when random weights are used like when `encoder_name` is `UNet` and when `backbone_weights` is `random`.\" \\\n",
        "        \"If the entire backbone is to be unfrozen, this should be `True`. Otherwise, `False` indicates the model is frozen or selected blocks (array of ints)\"\n",
        "    )\n",
        "    learning_rate: PositiveFloat = Field(\n",
        "        default=1e-4,\n",
        "        lt=1.0,\n",
        "        description=\"Learning rate for the Adam optimizer. Should be between 0 and 1\"\n",
        "    )\n",
        "    L2_regularization_strength: NonNegativeFloat = Field(\n",
        "        default=0.0,\n",
        "        lt=1.0,\n",
        "        description=\"Strength of L2 regularization used during training. Should be between 0 and 1, with 0 meaning no L2 regularization is used\"\n",
        "    )\n",
        "    batch_size: PositiveInt = Field(\n",
        "        default=4,\n",
        "        description=\"Number of image-annotation pairs to use in a single batch. Each batch represents one weight vector update\"\n",
        "    )\n",
        "    num_epochs: PositiveInt = Field(\n",
        "        default=50,\n",
        "        description=\"Number of epochs to train for. Each epoch is one pass through the entire dataset\"\n",
        "    )\n",
        "    augment: bool = Field(\n",
        "        default=True,\n",
        "        description=\"Augment the training subset eightfold by flipping and rotating by 90 deg intervals\"\n",
        "    )\n",
        "    cross_validation: bool = Field(\n",
        "        default=False,\n",
        "        description=\"Performs a k-fold cross validation study where many models are trained using different non-overlapping validation sets\"\n",
        "    )\n",
        "    num_folds: Optional[PositiveInt] = Field(\n",
        "        default=None,\n",
        "        gt=1,\n",
        "        description=\"Number of models to train for cross validation. Should be ignored when `cross_validation` is `false`\"\n",
        "    )\n",
        "    early_stopping: Optional[bool] = Field(\n",
        "        default=None,\n",
        "        description=\"Stop the training if the validation loss does not improve after a given number of epochs provided by `patience`. \" \\\n",
        "        \"Does not apply when `cross_validation` is `true`\"\n",
        "    )\n",
        "    patience: Optional[PositiveInt] = Field(\n",
        "        default=None,\n",
        "        description=\"Number of epochs before training is stopped automatically. Only applies when `early_stopping` is `true`\"\n",
        "    )\n",
        "    training_set: Optional[List[str]] = Field(\n",
        "        default=None,\n",
        "        description=\"Array of image filenames to be used for the training set. Reference the logical tree to see how it may be defined\"\n",
        "    )\n",
        "    validation_set: Optional[List[str]] = Field(\n",
        "        default=None,\n",
        "        description=\"Array of image filenames to be used for the validation set. Reference the logical tree to see how it may be defined\"\n",
        "    )\n",
        "    auto_split: Optional[PositiveFloat] = Field(\n",
        "        default=None, \n",
        "        lt=1,\n",
        "        description=\"Validation hold out percentage used when automatically splitting the dataset. Reference the logical tree to see how it may be defined\"\n",
        "    )\n",
        "    model_summary: bool = Field(\n",
        "        default=True,\n",
        "        description=\"Print out the model summary from Keras to a log file in the results directory\"\n",
        "    )\n",
        "    batchnorm: bool = Field(\n",
        "        default=False,\n",
        "        description=\"Option to use batch normalization after convolution layers in the UNet encoder/decoder\"\n",
        "    )\n",
        "\n",
        "    @model_validator(mode='after')\n",
        "    def pretrained_backbone(self) -> 'Train':\n",
        "        \"\"\"Validation specific to pretrained backbones (EfficientNet)\"\"\"\n",
        "        \n",
        "        # set the encoder filters if not supplied and not using EfficientNet encoder\n",
        "        if self.encoder_name == 'UNet' and self.encoder_filters is None:\n",
        "            self.encoder_filters=[64, 128, 256, 512, 1024]\n",
        "        \n",
        "        # backbone weights should only be specified when using EfficientNet\n",
        "        if self.encoder_name == 'UNet' and self.backbone_weights is not None:\n",
        "            self.backbone_weights == None\n",
        "            warn(\"`backbone_weights` should be `null` or ignored when `encoder_name` is `UNet`\")\n",
        "        elif self.encoder_name == 'EfficientNetB7' and self.backbone_weights is None: \n",
        "            raise ValueError(f\"`backbone_weights` should be `random` or `imagenet` when `encoder_name` is `EfficientNetB7`\")\n",
        "        \n",
        "        # backbone finetuning should be none when using random weights (UNet or random EfficientNet)\n",
        "        if self.encoder_name == 'UNet' and self.backbone_finetuning is not None:\n",
        "            self.backbone_finetuning = None\n",
        "            warn(\"`backbone_finetuning` should be `null` or ignored when `encoder_name` is `UNet`\")\n",
        "        elif self.encoder_name == 'EfficientNetB7' and self.backbone_weights == 'random' and self.backbone_finetuning is not None:\n",
        "            self.backbone_finetuning = None\n",
        "            warn(\"`backbone_finetuning` should be `null` when `backbone_weights` is `random`\")\n",
        "\n",
        "        # block level unfreezing should be an array of block ints (0,1,2,...,7) \n",
        "        if self.encoder_name == 'EfficientNetB7' and type(self.backbone_finetuning) == list:\n",
        "            assert len(self.backbone_finetuning) < 8 and len(self.backbone_finetuning) > 0, \"There must be at least 1 and at most 7 block indices in `backbone_finetuning`\"\n",
        "            assert len(set(self.backbone_finetuning)) == len(self.backbone_finetuning), \"All block indices must be unique in `backbone_finetuning`\"\n",
        "            assert np.all(np.array(self.backbone_finetuning) < 8), \"Block indices must be from 0 to 7 in `backbone_finetuning`\"\n",
        "\n",
        "        # default value is True (unfrozen backbone) if null\n",
        "        if self.encoder_name == 'EfficientNetB7' and self.backbone_finetuning is None and self.backbone_weights == 'imagenet':\n",
        "            warn(f\"Expected `backbone_finetuning` to be an `array`, `true`, or `false`. Got `null` and defaulting to `True` (unfrozen)\")\n",
        "            self.backbone_finetuning = True\n",
        "\n",
        "        return self\n",
        "\n",
        "    @field_validator('dataset_name', mode='after')\n",
        "    @classmethod\n",
        "    def check_dataset(cls, dataset_name : str) -> str:\n",
        "        \"\"\"Checks various aspects about the dataset name provided\"\"\"\n",
        "        \n",
        "        # check if the dataset directory exists\n",
        "        abs_path = ROOT_DIR / 'data' / dataset_name\n",
        "        if not abs_path.exists():\n",
        "            raise ValueError(f\"Dataset can not be found at `{abs_path}`\")\n",
        "\n",
        "        # check if the dataset has the proper subdirectories\n",
        "        img_subdir = abs_path / 'images'\n",
        "        ann_subdir = abs_path / 'annotations'\n",
        "        if not img_subdir.exists():\n",
        "            raise ValueError(f\"Dataset is missing the `images/` subdirectory\")\n",
        "        elif not ann_subdir.exists():\n",
        "            raise ValueError(f\"Dataset is missing the `annotations/` subdirectory\")\n",
        "        \n",
        "        # check if the dataset subdirectories have no child directories themselves (only files)\n",
        "        img_childdirs = [path.is_dir() for path in img_subdir.iterdir()]\n",
        "        ann_childdirs = [path.is_dir() for path in ann_subdir.iterdir()]\n",
        "        if any(img_childdirs):\n",
        "            raise ValueError(\"`images/` subdirectory should contain files, not directories\")\n",
        "        elif any(ann_childdirs):\n",
        "            raise ValueError(\"`annotations/` subdirectory should contain files, not directories\")\n",
        "            \n",
        "        # check if the dataset subdirectories have at least 2 files\n",
        "        img_files = list(img_subdir.iterdir())\n",
        "        ann_files = list(ann_subdir.iterdir())\n",
        "        num_imgs = len(img_files)\n",
        "        num_anns = len(ann_files)\n",
        "        if num_imgs != num_anns:\n",
        "            raise ValueError(\n",
        "                f\"There must be the same number of images and annotations. Got {num_imgs} image files and {num_anns} annotation files\")\n",
        "        elif num_imgs < 2:\n",
        "            raise ValueError(\"There must be at least 2 image/annotation file pairs\")\n",
        "        elif len(set(img_files)) != num_imgs: # set removes duplicates\n",
        "            raise ValueError(\"Every image/annotation filename must be unique\")\n",
        "        \n",
        "        # check if image/annotations have mixed file types \n",
        "        img_ext = {file.suffix for file in img_files}\n",
        "        ann_ext = {file.suffix for file in ann_files}\n",
        "        if len(img_ext) != 1:\n",
        "            raise ValueError(f'Images must have the same file type. Got types {img_ext}')\n",
        "        elif len(ann_ext) != 1:\n",
        "            raise ValueError(f'Annotations must have the same file type. Got types {ann_ext}')\n",
        "        \n",
        "        # make sure image/annotations are are JPEGs or PNGs\n",
        "        img_ext = next(iter(img_ext)) # gets element of singleton set\n",
        "        ann_ext = next(iter(ann_ext))\n",
        "        allowed_file_types = ['.jpg', '.jpeg', '.png']\n",
        "        if img_ext not in allowed_file_types:\n",
        "            raise ValueError(f'Expected image filetype to be one of {allowed_file_types}, got {img_ext}')\n",
        "        if ann_ext not in allowed_file_types:\n",
        "            raise ValueError(f'Expected annotation filetype to be one of {allowed_file_types}, got {ann_ext}')\n",
        "               \n",
        "        # check if every image has a corresponding annotation (by name)\n",
        "        img_stems = {file.stem for file in img_files}\n",
        "        ann_stems = {file.stem for file in ann_files}\n",
        "        if img_stems != ann_stems:\n",
        "            unpaired_stems = img_stems ^ ann_stems # get disjointed elements (unique to each set)\n",
        "            raise ValueError(f'Found unpaired image or annotation files with filenames {unpaired_stems}')\n",
        "        \n",
        "        return dataset_name\n",
        "    \n",
        "    @field_validator('batch_size', mode='after')\n",
        "    @classmethod\n",
        "    def batch_size_warning(cls, batch_size : int) -> int:\n",
        "        \"\"\"Warn the user if batch size is not a power of 2\"\"\"\n",
        "        if np.log2(batch_size) % 1 != 0.0:\n",
        "            warn(\"`batch_size` is not a power of two. Efficiency may be reduced\")\n",
        "        \n",
        "        return batch_size\n",
        "    \n",
        "    @model_validator(mode='after')\n",
        "    def check_early_stopping(self) -> 'Train':\n",
        "        \"\"\"Checks early_stopping and patience fields and validate when doing cross validation\"\"\"\n",
        "        \n",
        "        # patience should only be provided when using early stopping and should be less than the number of epochs\n",
        "        if self.early_stopping == False and self.patience is not None:\n",
        "            self.patience = None\n",
        "            warn(f\"`patience` should be `null` if `early_stopping` is `false`. Changed `patience` to `null`\")\n",
        "        elif self.early_stopping == True:\n",
        "            assert self.patience < self.num_epochs, \"`patience` can not be greater than `num_epochs`\"\n",
        "\n",
        "        # early_stopping should be null when doing cross validation\n",
        "        if self.cross_validation is True:\n",
        "            if self.early_stopping is not None or self.patience is not None:\n",
        "                self.early_stopping = None\n",
        "                self.patience = None\n",
        "                warn(\"`early_stopping` and `patience` should be `null` or ignored when `cross_validation` is `true`\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    @model_validator(mode='after')\n",
        "    def check_train_val(self) -> 'Train':\n",
        "        \"\"\"Checks many aspects of the train-val splitting when training single models and doing cross validation. It applies the logical tree from the docs\"\"\"\n",
        "        \n",
        "        # dataset has already been validated since field_validators run first\n",
        "        data_dir = ROOT_DIR / 'data' / self.dataset_name / 'images'\n",
        "        img_stems = [file.stem for file in data_dir.iterdir()]\n",
        "        img_stems_set = set(img_stems)\n",
        "\n",
        "        # basic cross validation check \n",
        "        if self.cross_validation == True:\n",
        "            # val sets will be determined algorithmically\n",
        "            assert self.validation_set is None, \"`validation_set` should be `null` or ignored when `cross_validation` is `true`\"\n",
        "\n",
        "        # logical tree with train at the top (see docs)\n",
        "\n",
        "        # ------------ FORK 1: train set provided or not ------------ #\n",
        "        if self.training_set is not None:\n",
        "            # make sure all train files exist\n",
        "            training_set = set(self.training_set)\n",
        "            check_files(training_set, img_stems_set, 'train')\n",
        "\n",
        "            # ------------ FORK 2: cross validation is true or not ------------ # \n",
        "            if self.cross_validation == True:\n",
        "                # check number of folds\n",
        "                assert self.num_folds < len(self.training_set), \"`num_folds` can not be greater than the number of training images used for cross validation\"\n",
        "\n",
        "            else:\n",
        "                # ------------ FORK 3: val set is provided or not ------------ # \n",
        "                if self.validation_set is not None:\n",
        "                    # check overlap between train and val sets and make sure val set files exist\n",
        "                    validation_set = set(self.validation_set)\n",
        "                    train_val_overlap = training_set & validation_set\n",
        "                    assert len(train_val_overlap) == 0, f\"Files with the names {train_val_overlap} were found in both `training_set` and `validation_set`\"\n",
        "                    check_files(validation_set, img_stems_set, 'validation')\n",
        "                else:\n",
        "\n",
        "                    # ------------ FORK 4: auto split is provided or not ------------ # \n",
        "                    if self.auto_split:\n",
        "                        # make sure auto_split is not too large where no train set is created\n",
        "                        assert self.auto_split < (1-(1/len(self.training_set))), f\"auto_split validation hold-out percentage must be less than {1-(1/len(self.training_set))} for the `training_set` provided\"\n",
        "                    else:\n",
        "                        # val set is the complement of train; make sure train does not have all available images\n",
        "                        assert training_set < img_stems_set, \"`training_set` can not have all images in the dataset. Some must be left over for `validation_set`\"\n",
        "                        # generate val set (sort them naturally)\n",
        "                        self.validation_set = os_sorted(list(img_stems_set - training_set))\n",
        "\n",
        "        # ------------ FORK 1: back to the top (train set not provided) ------------ # \n",
        "        else:\n",
        "\n",
        "            # ------------ FORK 2: cross validation is true or not ------------ # \n",
        "            if self.cross_validation == True:\n",
        "                self.training_set = img_stems\n",
        "            else:\n",
        "\n",
        "                # ------------ FORK 3: val_set is provided or not ------------ #\n",
        "                if self.validation_set is not None:\n",
        "                    # train set is the complement to val set\n",
        "                    validation_set = set(self.validation_set)\n",
        "                    assert validation_set < img_stems_set, \"`validation_set` can not have all images in the dataset. Some must be left over for `training_set`\"\n",
        "                    # generate train set (sort them naturally)\n",
        "                    self.training_set = os_sorted(list(img_stems_set - validation_set))\n",
        "                else:\n",
        "                    # define auto_split if it is not provided, or just check its value\n",
        "                    if not self.auto_split:\n",
        "                        self.auto_split = 0.4\n",
        "                        warn(\"`auto_split` validation hold-out percentage not provided even though `training_set` and `validation_set` are `null` and `cross_validation` is `false`. Defaulting to 40%\")\n",
        "                    else:      \n",
        "                        assert self.auto_split < (1-1/len(img_stems_set)), f\"`auto_split` validation hold-out percentage must be less {1-(1/len(img_stems_set))} for the `dataset_name` provided\"\n",
        "\n",
        "        return self\n",
        "    \n",
        "    @model_validator(mode='after')\n",
        "    def generate_results_dir(self) -> 'Train':\n",
        "        \"\"\"Create the results directory following a naming scheme if one is not provided\"\"\"\n",
        "\n",
        "        if self.results_dir is None:\n",
        "            now = datetime.now()\n",
        "            self.results_dir = 'results_' + self.dataset_name + '_' + self.operation_mode + '_' + self.encoder_name + '_' + self.decoder_name \n",
        "            if self.cross_validation:\n",
        "                self.results_dir += '_crossval'\n",
        "            self.results_dir += now.strftime('_(%Y-%m-%d)_(%H-%M-%S)')\n",
        "        \n",
        "        self.results_dir = ROOT_DIR / self.results_dir\n",
        "\n",
        "        return self\n",
        "    \n",
        "    @model_validator(mode='after')\n",
        "    def get_image_shape(self) -> 'Train':\n",
        "        \"\"\"Get the image shape for model instantiation and make sure it is consistent\"\"\"\n",
        "        \n",
        "        data_dir = ROOT_DIR / 'data' / self.dataset_name\n",
        "\n",
        "        # load each image or annotation and get the array shape; len(set)=1is found\n",
        "        img_shapes = {cv.imread(str(img_path), cv.IMREAD_COLOR).shape for img_path in (data_dir / 'images').iterdir()}\n",
        "        ann_shapes = {cv.imread(str(ann_path), cv.IMREAD_COLOR).shape for ann_path in (data_dir / 'annotations').iterdir()}\n",
        "        img_shape = next(iter(img_shapes))\n",
        "        ann_shape = next(iter(ann_shapes))\n",
        "\n",
        "        # make sure all images and annotations have only one shape \n",
        "        if len(img_shapes) > 1:\n",
        "            raise KeyError(f\"Expected all images to have the same shape. Got shapes {img_shapes}\")\n",
        "        elif len(ann_shapes) > 1:\n",
        "            raise KeyError(f\"Expected all annotations to have the same shape. Got shapes {ann_shapes}\")\n",
        "        elif img_shape != ann_shape:\n",
        "            raise KeyError(f\"Expected images and annotations to have the same shape. Got an image shape of `{img_shape}` and an annotation shape of `{ann_shape}`\")\n",
        "        else:\n",
        "            self.input_shape = img_shape\n",
        "\n",
        "        return self\n",
        "\n",
        "            \n",
        "class Inference(BaseModel):\n",
        "    root_dir: Path\n",
        "    operation_mode: str\n",
        "    dataset_name: str\n",
        "    results_dir: Optional[str | Path]\n",
        "    model_path: str = Field(\n",
        "        description=\"Path relative to /path/to/unet-compare/ to an existing model to be used for inference\"\n",
        "    )\n",
        "    \n",
        "    @field_validator('dataset_name', mode='after')\n",
        "    @classmethod\n",
        "    def check_dataset(cls, dataset_name : str) -> str:\n",
        "        \"\"\"Checks various aspects about the dataset name provided\"\"\"\n",
        "        \n",
        "        # check if the dataset directory exists\n",
        "        abs_path = ROOT_DIR / 'data' / dataset_name\n",
        "        if not abs_path.exists():\n",
        "            raise ValueError(f\"Dataset can not be found at `{abs_path}`\")\n",
        "        \n",
        "        # check if the dataset directory has no child directories (only files)\n",
        "        childdirs = [path.is_dir() for path in abs_path.iterdir()]\n",
        "        if any(childdirs):\n",
        "            raise ValueError(\"Dataset subdirectory should contain files, not directories\")\n",
        "            \n",
        "        # check if the dataset has at least 1 file\n",
        "        img_files = list(abs_path.iterdir())\n",
        "        num_imgs = len(img_files)\n",
        "        if not num_imgs:\n",
        "            raise ValueError(f\"There must be at least 1 image to inference in the dataset directory\")\n",
        "        \n",
        "        # make sure images are JPEGs or PNGs\n",
        "        img_exts = {file.suffix for file in img_files}\n",
        "        allowed_file_types = ['.jpg', '.jpeg', '.png']\n",
        "        unallowed_files = []\n",
        "        for file in img_files:\n",
        "            if file.suffix not in allowed_file_types:\n",
        "                unallowed_files.append(file)\n",
        "        if len(unallowed_files):\n",
        "            raise ValueError(f'The files {unallowed_files} have invalid file types')\n",
        "        \n",
        "        # make sure images have the same file type\n",
        "        if len(img_exts) > 1:\n",
        "            raise KeyError(f\"Expected all files to have the same file type. Got mixed types {img_exts}\")\n",
        "    \n",
        "        return dataset_name\n",
        "\n",
        "    @field_validator('model_path', mode='after')\n",
        "    @classmethod\n",
        "    def check_model(cls, model_path) -> 'General':\n",
        "        # check if the model file exists\n",
        "        abs_path = ROOT_DIR / model_path\n",
        "        if not abs_path.exists():\n",
        "            raise ValueError(f'No model file exists at `{abs_path}`')\n",
        "\n",
        "        # check if it is a .keras model file\n",
        "        ext = abs_path.name.split('.', 1)[-1]\n",
        "        if ext == 'weights.h5':\n",
        "            raise ValueError('Expected a .keras model file, got a weights.h5 file instead')\n",
        "        elif ext == 'keras':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f'Expected a .keras file, got a .{ext} file')\n",
        "        \n",
        "        return model_path\n",
        "\n",
        "    @model_validator(mode='after')\n",
        "    def generate_results_dir(self) -> 'Train':\n",
        "        \"\"\"Create the results directory following a naming scheme if one is not provided\"\"\"\n",
        "\n",
        "        if self.results_dir is None:\n",
        "            now = datetime.now()\n",
        "            self.results_dir = 'results_' + self.dataset_name + '_' + self.operation_mode + now.strftime('_(%Y-%m-%d)_(%H-%M-%S)')\n",
        "        \n",
        "        self.results_dir = ROOT_DIR / self.results_dir\n",
        "\n",
        "        return self\n",
        "    \n",
        "\n",
        "def check_files(file_set: set, master_set: set, set_type: str):\n",
        "    \"\"\"Checks if all files in a set exist in a provided directory\"\"\"\n",
        "    if not file_set <= master_set:\n",
        "        missing_fns = file_set - master_set\n",
        "        raise ValueError(f\"Could not find files with the names {missing_fns} in the dataset given in `{set_type}_set\")\n",
        "\n",
        "\n",
        "def validate(input_configs: dict) -> dict:\n",
        "    \"\"\"Generate Pydantic models and validate input\"\"\"\n",
        "    general = General.model_validate(input_configs)\n",
        "\n",
        "    # select validator specific to the operation mode\n",
        "    if general.operation_mode == 'train':\n",
        "        output_configs = Train.model_validate(general.model_dump())\n",
        "    else:\n",
        "        output_configs = Inference.model_validate(general.model_dump())\n",
        "\n",
        "    return output_configs.model_dump()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92NVEcYAKbd_"
      },
      "source": [
        "## Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6Z4J7efKaN3"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aiden Ochoa, 4/2025, RDMAP PSU Research Group\n",
        "This module handles the definition of the custom convolution layers used in UNet models\n",
        "\"\"\"\n",
        "\n",
        "def ConvBlock(inputs, filters, batchnorm, l2_reg, index):\n",
        "    \"\"\"Two Conv2D with optional batchnorm layer and ReLU activation\"\"\"\n",
        "\n",
        "    def ConvUnit(inputs, layer_index):\n",
        "        x = Conv2D(filters, 3, padding='same', name='conv_'+layer_index, use_bias=not(batchnorm), kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg))(inputs)\n",
        "        if batchnorm:\n",
        "            x = BatchNormalization(name='bn_'+layer_index)(x)\n",
        "        return Activation('relu', name='relu_'+layer_index)(x)\n",
        "\n",
        "    x = ConvUnit(inputs, index+'a')\n",
        "    return ConvUnit(x, index+'b')\n",
        "\n",
        "\n",
        "def UpsampleBlock(inputs, filters, batchnorm, l2_reg, index):\n",
        "    \"\"\"Conv2DTranspose layer for upsampling with optional batchnorm layer and ReLU activation\"\"\"\n",
        "    \n",
        "    x = Conv2DTranspose(filters, 2, padding='same', name='up_'+index, use_bias=not(batchnorm), kernel_initializer='he_normal', kernel_regularizer=l2(l2_reg), strides=2)(inputs)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization(name='bn_'+index)(x)\n",
        "    return Activation('relu', name='relu_'+index)(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rr6ZvWOKpVN"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj-B_wMpKqxe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aiden Ochoa, 4/2025, RDMAP PSU Research Group\n",
        "This module handles dataset processing for training and inference\n",
        "\"\"\"\n",
        "\n",
        "def create_train_dataset(configs: dict) -> dict:\n",
        "    \"\"\"Creates the training dataset based on the configs\"\"\"\n",
        "\n",
        "    data_dir: Path = configs['root_dir'] / 'data' / configs['dataset_name']\n",
        "    dataset_dir: Path = configs['root_dir'] / 'dataset'\n",
        "\n",
        "    # Step 1: remove existing directory if it exists\n",
        "    if dataset_dir.exists():\n",
        "        shutil.rmtree(dataset_dir)\n",
        "\n",
        "    # Step 2: generate train/val filename lists\n",
        "    configs['training_set'], configs['validation_set'] = split_data(configs)\n",
        "\n",
        "    # Step 3: convert filename stems to full paths\n",
        "    img_ext = next(iter({file.suffix for file in (data_dir / 'images').iterdir()}))\n",
        "    ann_ext = next(iter({file.suffix for file in (data_dir / 'annotations').iterdir()}))\n",
        "\n",
        "    train_img_paths = [data_dir / 'images' / (file + img_ext) for file in configs['training_set']]\n",
        "    val_img_paths = [data_dir / 'images' / (file + img_ext) for file in configs['validation_set']]\n",
        "    train_ann_paths = [data_dir / 'annotations' / (file + ann_ext) for file in configs['training_set']]\n",
        "    val_ann_paths = [data_dir / 'annotations' / (file + ann_ext) for file in configs['validation_set']]\n",
        "\n",
        "    # Step 4: populate dataset tree\n",
        "    copy_files(train_img_paths, dataset_dir / 'images' / 'train')\n",
        "    copy_files(val_img_paths, dataset_dir / 'images' / 'val')\n",
        "    copy_files(train_ann_paths, dataset_dir / 'annotations' / 'train')\n",
        "    copy_files(val_ann_paths, dataset_dir / 'annotations' / 'val')\n",
        "\n",
        "    # Step 5: augment training set\n",
        "    if configs['augment']:\n",
        "        augment_dataset(dataset_dir / 'images' / 'train', img_ext, ann_ext)\n",
        "\n",
        "    # Step 6: create Dataset Tensors\n",
        "    train_dataset = tf.data.Dataset.list_files(str(dataset_dir / 'images' / 'train' / '*'))\n",
        "    val_dataset = tf.data.Dataset.list_files(str(dataset_dir / 'images' / 'val' / '*'))\n",
        "\n",
        "    # replace each string with a tuple that has elements (image_tensor, annotation_tensor, image_path_tensor)\n",
        "    train_dataset = train_dataset.map(lambda x: parse_image(x, img_ext, ann_ext), num_parallel_calls=AUTOTUNE)\n",
        "    val_dataset = val_dataset.map(lambda x: parse_image(x, img_ext, ann_ext), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    BUFFER_SIZE = 48\n",
        "\n",
        "    # shuffle the training dataset and batch it\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE)\n",
        "    train_dataset = train_dataset.repeat()\n",
        "    train_dataset = train_dataset.batch(configs['batch_size'])\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    # batch the validation dataset\n",
        "    val_dataset = val_dataset.repeat()\n",
        "    val_dataset = val_dataset.batch(1)\n",
        "    val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "    # determine number of steps to take in an epoch\n",
        "    train_steps = len(list((dataset_dir / 'images' / 'train').iterdir())) // configs['batch_size']\n",
        "    val_steps = len(list((dataset_dir / 'images' / 'val').iterdir())) // 1\n",
        "\n",
        "    return {'train_dataset' : train_dataset, 'val_dataset' : val_dataset, 'train_steps' : train_steps, 'val_steps' : val_steps}\n",
        "\n",
        "\n",
        "def create_train_val_inference_dataset(configs: dict) -> dict:\n",
        "\n",
        "    data_dir: Path = configs['root_dir'] / 'data' / configs['dataset_name']\n",
        "    img_ext = next(iter({file.suffix for file in (data_dir / 'images').iterdir()}))\n",
        "\n",
        "    # Step 1: convert filenames to full paths\n",
        "    train_paths = [data_dir / 'images' / (file + img_ext) for file in configs['training_set']]\n",
        "    val_paths = [data_dir / 'images' / (file + img_ext) for file in configs['validation_set']]\n",
        "\n",
        "    # Step 2: convert full paths list to tensor\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices([str(path) for path in train_paths])\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices([str(path) for path in val_paths])\n",
        "\n",
        "    # Step 3: load images and batch them\n",
        "    train_dataset = train_dataset.map(lambda x: parse_image(x, img_ext, None), num_parallel_calls=AUTOTUNE)\n",
        "    val_dataset = val_dataset.map(lambda x: parse_image(x, img_ext, None), num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "    train_dataset = train_dataset.batch(1)\n",
        "    val_dataset = val_dataset.batch(1)\n",
        "\n",
        "    return {'train_dataset' : train_dataset, 'train_paths' : train_paths, 'val_dataset' : val_dataset, 'val_paths' : val_paths}\n",
        "\n",
        "\n",
        "def create_inference_dataset(configs: dict) -> dict[tf.Tensor, list[Path]]:\n",
        "     # define path to images\n",
        "    data_dir= configs['root_dir'] / 'data' / configs['dataset_name']\n",
        "    data_paths = [path for path in data_dir.iterdir()]\n",
        "    img_ext = next(iter(set(data_paths)))\n",
        "\n",
        "    # create tensorflow dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices([str(path) for path in data_paths])\n",
        "    dataset = dataset.map(lambda x: parse_image(x, img_ext, None), num_parallel_calls=AUTOTUNE)\n",
        "    dataset = dataset.batch(1)\n",
        "\n",
        "    return {'dataset' : dataset, 'data_paths' : data_paths}\n",
        "\n",
        "\n",
        "\"\"\"Miscellaneous functions\"\"\"\n",
        "\n",
        "\n",
        "def split_data(configs: dict) -> Tuple[List[str], List[str]]:    \n",
        "    # get current state\n",
        "    data_dir = configs['root_dir'] / 'data' / configs['dataset_name'] / 'images'\n",
        "    all_fns = [file.stem for file in data_dir.iterdir()]\n",
        "    train_fns, val_fns = configs['training_set'], configs['validation_set']\n",
        "\n",
        "    # Case 1: only training files provided\n",
        "    if (train_fns is not None) and (val_fns is None):\n",
        "        # split it with the given percentage\n",
        "        if configs['auto_split']:\n",
        "            random.shuffle(train_fns)\n",
        "            train_upper = int(len(train_fns)*(1-configs['auto_split']))\n",
        "            return train_fns[:train_upper], train_fns[train_upper:]\n",
        "        # or use remaining files\n",
        "        else:\n",
        "            val_fns = os_sorted(list(set(all_fns) - set(train_fns)))\n",
        "            return train_fns, val_fns\n",
        "    \n",
        "    # Case 2: only validation files provided\n",
        "    elif (train_fns is None) and (val_fns is not None):\n",
        "        # use remaining files\n",
        "        train_fns = os_sorted(list(set(all_fns) - set(val_fns)))\n",
        "        return train_fns, val_fns\n",
        "    \n",
        "    # Case 3: both training and validation files provided\n",
        "    elif (train_fns is not None) and (val_fns is not None):\n",
        "        # do nothing\n",
        "        return train_fns, val_fns\n",
        "\n",
        "    # Case 4: neither training nor validation files provided\n",
        "    else:\n",
        "        # split it with the given percentage (40% is default)\n",
        "        random.shuffle(all_fns)\n",
        "        train_upper = int(len(all_fns)*(1-configs['auto_split']))\n",
        "        return all_fns[:train_upper], all_fns[train_upper:]\n",
        "\n",
        "\n",
        "def copy_files(file_paths: list[Path], dest_dir: Path):\n",
        "    if not dest_dir.exists():\n",
        "        dest_dir.mkdir(parents=True)\n",
        "       \n",
        "    for path in file_paths:\n",
        "        shutil.copy(path, dest_dir / path.name)\n",
        "\n",
        "\n",
        "def augment_dataset(train_image_dir: Path, img_ext: str, ann_ext: str):\n",
        "    # loop through training images\n",
        "    for img in glob(str(train_image_dir / '*')):\n",
        "        # replace images in path with annotations and image extension to annotation extension\n",
        "        ann = re.sub('images', 'annotations', img)\n",
        "        ann = re.sub(img_ext, ann_ext, ann)\n",
        "\n",
        "        # augment image and annotation\n",
        "        augment_single_image(Path(img))\n",
        "        augment_single_image(Path(ann))\n",
        "\n",
        "\n",
        "def augment_single_image(path : Path):\n",
        "    # load file\n",
        "    image = cv.imread(str(path))\n",
        "\n",
        "    # perform transformations on image\n",
        "    image_1 = path.parent / (path.stem + '_1' + path.suffix)    # original (just change its name)\n",
        "    image_2 = cv.rotate(image, cv.ROTATE_90_CLOCKWISE)          # rot90\n",
        "    image_3 = cv.rotate(image, cv.ROTATE_180)                   # rot180\n",
        "    image_4 = cv.rotate(image, cv.ROTATE_90_COUNTERCLOCKWISE)   # rot270\n",
        "    image_5 = cv.flip(image, 1)                                 # xflip\n",
        "    image_6 = cv.flip(image_2, 1)                               # rot90 + xflip\n",
        "    image_7 = cv.flip(image_2, 0)                               # rot90 + yflip\n",
        "    image_8 = cv.flip(image_3, 1)                               # rot180 + xflip\n",
        "\n",
        "    # save augmentations\n",
        "    path.rename(image_1)\n",
        "    cv.imwrite(str(path.parent / (path.stem + '_2' + path.suffix)), image_2)\n",
        "    cv.imwrite(str(path.parent / (path.stem + '_3' + path.suffix)), image_3)\n",
        "    cv.imwrite(str(path.parent / (path.stem + '_4' + path.suffix)), image_4)\n",
        "    cv.imwrite(str(path.parent / (path.stem + '_5' + path.suffix)), image_5)\n",
        "    cv.imwrite(str(path.parent / (path.stem + '_6' + path.suffix)), image_6)\n",
        "    cv.imwrite(str(path.parent / (path.stem + '_7' + path.suffix)), image_7)\n",
        "    cv.imwrite(str(path.parent / (path.stem + '_8' + path.suffix)), image_8)\n",
        "\n",
        "\n",
        "def parse_image(img_path: tf.Tensor, img_ext: str, ann_ext: str) -> tuple[tf.Tensor, tf.Tensor]:\n",
        "    # read image and load it into 3 channels (pre-trained backbones require 3) and normalize it\n",
        "    image = tf.io.read_file(img_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    \n",
        "    try:\n",
        "        # adjust path to lead to the corresponding annotation and load it\n",
        "        ann_path = tf.strings.regex_replace(img_path, 'images', 'annotations')\n",
        "        ann_path = tf.strings.regex_replace(ann_path, img_ext, ann_ext)\n",
        "        annotation = tf.io.read_file(ann_path)\n",
        "        annotation = tf.image.decode_png(annotation, channels=1)\n",
        "        annotation = tf.cast(annotation, tf.float32) / 255.0\n",
        "        return image, annotation\n",
        "    except:\n",
        "        # if ann_ext is None, we just want the image\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHDNKtUiF1RO"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-fqc_OgK9Wh"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aiden Ochoa, 4/2025, RDMAP PSU Research Group\n",
        "This module handles the definition of encoder/decoder subnetworks and their connection\n",
        "\"\"\"\n",
        "\n",
        "def load_UNet(configs : dict) -> keras.Model:\n",
        "    \"\"\"U-Net built with Functional API using either U-Net or EfficientNetB7 encoders and either U-Net or U-Net++ decoders\"\"\"\n",
        "\n",
        "    enc_filters = configs['encoder_filters']\n",
        "    dec_filters = configs['decoder_filters']\n",
        "    batchnorm = configs['batchnorm']\n",
        "    l2_reg = configs['L2_regularization_strength']\n",
        "\n",
        "    input = keras.Input(shape = configs['input_shape'], name = 'main_input')\n",
        "\n",
        "    # encoder\n",
        "    if configs['encoder_name'] == 'UNet':\n",
        "        model_name = 'UNet'\n",
        "        x = input\n",
        "        enc_outputs = []\n",
        "        # repeatedly adds a convolution layer and saves the current outputs\n",
        "        for idx, filters in enumerate(enc_filters):\n",
        "            name_idx = f'{idx}0'\n",
        "            # conv\n",
        "            x = ConvBlock(x, filters, batchnorm, l2_reg, name_idx)\n",
        "            enc_outputs.append(x)\n",
        "            # pool except for the last block\n",
        "            if idx < 4:\n",
        "                x = MaxPooling2D(pool_size=2, name = 'pool_'+name_idx)(x)\n",
        "        \n",
        "    elif configs['encoder_name'] == 'EfficientNetB7':\n",
        "        model_name = 'EfficientNetB7'\n",
        "        if configs['backbone_weights'] == 'random':\n",
        "            weights = None\n",
        "        else:\n",
        "            weights = 'imagenet'\n",
        "        backbone = EfficientNetB7(include_top = False, weights = weights, input_tensor = input)\n",
        "\n",
        "        # handles model freezing (batchnorm layers must always stay frozen)\n",
        "        # freeze backbone\n",
        "        if configs['backbone_finetuning'] == False:\n",
        "            for layer in backbone.layers:\n",
        "                layer.trainable = False\n",
        "        else:\n",
        "            # freeze specific blocks (or leave the whole model unfrozen if not a list)\n",
        "            if type(configs['backbone_finetuning']) == list:\n",
        "                block_strs = ['block' + str(block_idx) for block_idx in configs['backbone_finetuning']]\n",
        "                for layer in backbone.layers:\n",
        "                    if layer.name[:6] not in block_strs:\n",
        "                        layer.trainable = False\n",
        "            # freeze batchnorm layers\n",
        "            for layer in backbone.layers:\n",
        "                if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "                    layer.trainable = False\n",
        "\n",
        "        enc_stages = ['stem_activation', 'block2g_add', 'block3g_add', 'block5j_add', 'block7d_add']\n",
        "        enc_outputs = [backbone.get_layer(stage).output for stage in enc_stages]\n",
        "    \n",
        "    # decoder\n",
        "    enc_outputs.reverse()\n",
        "    if configs['decoder_name'] == 'UNet':\n",
        "        model_name += '-UNet'\n",
        "        x = enc_outputs[0]\n",
        "        for idx, filters in enumerate(dec_filters[:-1]):\n",
        "            name_idx = f'{3-idx}{idx+1}' # 31, 22, 13, 04\n",
        "            x = UpsampleBlock(x, dec_filters[idx], batchnorm, l2_reg, name_idx)\n",
        "            x = Concatenate(name='cat_'+name_idx)([x, enc_outputs[idx+1]])\n",
        "            x = ConvBlock(x, dec_filters[idx], batchnorm, l2_reg, name_idx)\n",
        "\n",
        "    elif configs['decoder_name'] == 'UNet++':\n",
        "        model_name += '-UNetpp'\n",
        "        prev_row_outs = [enc_outputs[0]]\n",
        "        for row in range(4): # number of rows\n",
        "            current_row_outs = [enc_outputs[row+1]]\n",
        "            for node in range(row+1): # 1, 2, 3, 4 nodes per row\n",
        "                name_idx = f'{3-row}{node+1}' # 31, 21, 22, 11, 12, 13, 01, 02, 03, 04\n",
        "                x = UpsampleBlock(prev_row_outs[node], dec_filters[row], batchnorm, l2_reg, name_idx)\n",
        "                x = Concatenate(name='cat_'+name_idx)([x] + current_row_outs[:(node+1)])\n",
        "                x = ConvBlock(x, dec_filters[row], batchnorm, l2_reg, name_idx)\n",
        "                current_row_outs.append(x)\n",
        "            prev_row_outs = current_row_outs\n",
        "\n",
        "    # final layers\n",
        "    if configs['encoder_name'] == 'EfficientNetB7':\n",
        "        x = UpsampleBlock(x, dec_filters[4], batchnorm, l2_reg, 'final') # upsamples back to original resolution\n",
        "    sigmoid = Conv2D(1, 1, activation='sigmoid', name='main_output', kernel_initializer='he_normal', padding='same', kernel_regularizer=l2(l2_reg))(x)\n",
        "\n",
        "    return keras.Model(inputs=input, outputs=sigmoid, name = model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoSwtPN-LKd8"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3tCnLzULQFs"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aiden Ochoa, 4/2025, RDMAP PSU Research Group\n",
        "This module handles all accessory operations such as plotting\n",
        "\"\"\"\n",
        "\n",
        "def save_preds(preds : list[np.array], save_paths : list[Path]):\n",
        "    \"\"\"Save inference predictions from model.predict()\"\"\"\n",
        "    \n",
        "    for i, pred in enumerate(preds):\n",
        "        save_img(str(save_paths[i]), pred)\n",
        "\n",
        "\n",
        "def print_save_configs(configs : dict):\n",
        "    \"\"\"Creates the results directory, prints the input configs after validation, and saves a copy to results\"\"\"\n",
        "    \n",
        "    # create top-level results directory\n",
        "    configs['results_dir'].mkdir()\n",
        "    \n",
        "    # print input to user for confirmation\n",
        "    print('-'*60 + ' User Input ' + '-'*60)\n",
        "    for key, val in configs.items():\n",
        "        print(key + ':', val)\n",
        "    print('-'*132)\n",
        "\n",
        "    # save configs into results dir for reference\n",
        "    with open(configs['results_dir'] / 'configs.json', 'w') as con:\n",
        "        # make Path objects strings for serialization\n",
        "        configs['root_dir'] = str(configs['root_dir'])\n",
        "        configs['results_dir'] = str(configs['results_dir'])\n",
        "        json.dump(configs, con)\n",
        "\n",
        "\n",
        "def plot_results(configs : dict):\n",
        "    \"\"\"Loads training metrics from a single training loop, plots them, and saves it to the results directory\"\"\"\n",
        "\n",
        "    # paths\n",
        "    metrics_path = configs['results_dir'] / 'metrics.csv'\n",
        "    plot_save_path = configs['results_dir'] / 'metrics.png'\n",
        "\n",
        "    # read metrics into dataframe\n",
        "    metrics = pd.read_csv(metrics_path)\n",
        "\n",
        "    # get num epochs, and redefine it offset by 1\n",
        "    num_epochs = metrics['epoch'].count()\n",
        "    metrics['epoch'] = metrics['epoch'] + 1\n",
        "\n",
        "    # determine lowest val loss index (where val loss is closest to min(val_loss))\n",
        "    best_idx = np.where(np.isclose(metrics['val_loss'], min(metrics['val_loss'])))[0]\n",
        "\n",
        "    # add f1 columns to the dataframe\n",
        "    metrics['f1'] = add_f1(metrics)\n",
        "    metrics['val_f1'] = add_f1(metrics, val = True)\n",
        "\n",
        "    # generate subplots\n",
        "    fig, axs = plt.subplots(4, 1, figsize=(12,20))\n",
        "\n",
        "    titles = ['BCE Loss', 'Precision', 'Recall', 'F1-Score']\n",
        "    y_axes = ['loss', 'Precision', 'Recall', 'f1']\n",
        "\n",
        "    for i in range(len(axs)):\n",
        "        y1 = y_axes[i]\n",
        "        y2 = 'val_' + y1\n",
        "\n",
        "        # plot metric curve\n",
        "        axs[i].plot(metrics['epoch'], metrics[y1], '-o',  label='Train')\n",
        "        axs[i].plot(metrics['epoch'], metrics[y2], '-o', label='Val')\n",
        "        \n",
        "        # add point corresponding to lowest val loss on each curve\n",
        "        axs[i].plot(best_idx + 1, metrics[y1].iloc[best_idx], 'D', color='purple')\n",
        "        axs[i].plot(best_idx + 1, metrics[y2].iloc[best_idx], 'D', color='purple', label='Min Val Loss')\n",
        "        \n",
        "        # misc settings\n",
        "        axs[i].set_xlabel('Epoch')\n",
        "        axs[i].set_ylabel(titles[i])\n",
        "        axs[i].set_xlim([1,num_epochs])\n",
        "        if i == 0:\n",
        "            axs[i].set_yscale('log')\n",
        "        else:\n",
        "            axs[i].set_yticks(ticks=np.arange(0,1.1,0.1))\n",
        "        axs[i].grid(visible=True)\n",
        "        axs[i].legend()     \n",
        "\n",
        "    fig.savefig(str(plot_save_path), bbox_inches=\"tight\")\n",
        "\n",
        "\n",
        "def add_f1(metrics : pd.DataFrame, val = False) -> pd.DataFrame:\n",
        "    \"\"\"Calculates the f1-score element-wise given columns of a Pandas metrics dataframe\"\"\"\n",
        "\n",
        "    if val == True:\n",
        "        p = 'val_Precision'\n",
        "        r = 'val_Recall'\n",
        "    else:\n",
        "        p = 'Precision'\n",
        "        r = 'Recall'\n",
        "    \n",
        "    # if the denominator is 0, then f1=0, otherwise it is the harmonic mean\n",
        "    return np.where(metrics[p] + metrics[r] == 0, 0, 2  * (metrics[p] * metrics[r]) / (metrics[p] + metrics[r]))\n",
        "\n",
        "\n",
        "def cv_plot_results(configs : dict):\n",
        "    \"\"\"Loads in metrics from every cross validation fold, plots loss curves together, and plots statistics for each epoch\"\"\"\n",
        "\n",
        "    # paths\n",
        "    loss_save_path = configs['results_dir'] / 'loss.png'\n",
        "    metrics_save_path = configs['results_dir'] / 'metrics.png'\n",
        "\n",
        "    # get fold directory names and sort them using natural sorting\n",
        "    fold_dirs = glob(str(configs['results_dir'] / 'fold_*'))\n",
        "    fold_dirs = os_sorted(fold_dirs)\n",
        "\n",
        "    # dict to hold dataframes for each fold\n",
        "    all_metrics = []\n",
        "    for fold in range(len(fold_dirs)):\n",
        "        # get metrics from csv\n",
        "        fold_metrics = pd.read_csv(str(Path(fold_dirs[fold]) / 'metrics.csv'))\n",
        "\n",
        "        # add f1 columns to the dataframe\n",
        "        fold_metrics['f1'] = add_f1(fold_metrics)\n",
        "        fold_metrics['val_f1'] = add_f1(fold_metrics, val = True)\n",
        "\n",
        "        # convert to np array and add metrics array to list \n",
        "        fold_metrics_np = fold_metrics.to_numpy()\n",
        "        all_metrics.append(fold_metrics_np)\n",
        "\n",
        "    # stack all metrics arrays along a new 3d axis\n",
        "    all_metrics = np.stack(all_metrics, axis=0)\n",
        "\n",
        "    # get epochs list (always the same)\n",
        "    epochs = all_metrics[0, :, 0].astype(int) + 1\n",
        "\n",
        "    # plot loss curves together on two separate plots\n",
        "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
        "\n",
        "    for fold in range(configs['num_folds']):\n",
        "        # train/val losses\n",
        "        train_loss = all_metrics[fold, :, 4]\n",
        "        val_loss = all_metrics[fold, :, 8]\n",
        "\n",
        "        # plot losses\n",
        "        axs[0].plot(epochs, train_loss, label = 'Fold {}'.format(fold+1))\n",
        "        axs[1].plot(epochs, val_loss, label = 'Fold {}'.format(fold+1))\n",
        "\n",
        "    # formatting\n",
        "    axs[0].set_ylabel('Train Loss (BCE)')\n",
        "    axs[1].set_ylabel('Val Loss (BCE)')\n",
        "    for ax in axs:\n",
        "        ax.set_yscale('log')   \n",
        "        ax.set_xlim([1, configs['num_epochs']])\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.grid(visible=True)\n",
        "        ax.legend()\n",
        "\n",
        "    fig.savefig(str(loss_save_path), bbox_inches=\"tight\")\n",
        "\n",
        "    # get mean and stdev across all folds\n",
        "    num_metrics = np.shape(all_metrics)[-1]\n",
        "    metrics_mean = np.zeros((configs['num_epochs'], num_metrics))\n",
        "    metrics_std = np.zeros((configs['num_epochs'], num_metrics))\n",
        "\n",
        "    for metric in range(num_metrics):\n",
        "        for epoch in range(configs['num_epochs']):\n",
        "            metrics_mean[epoch, metric] = np.mean(all_metrics[:, epoch, metric])\n",
        "            metrics_std[epoch, metric] = np.std(all_metrics[:, epoch, metric])\n",
        "\n",
        "    # plot averaged metrics with std as error bars\n",
        "    fig, axs = plt.subplots(4, 1, figsize=(12, 20))\n",
        "\n",
        "    # settings specific to each plot \n",
        "    titles = ['BCE Loss', 'Precision', 'Recall', 'F1-Score']\n",
        "    train_metrics_idcs = [4, 1, 2, 9]\n",
        "    val_metrics_idcs = [8, 5, 6, 10]\n",
        "\n",
        "    for i in range(len(axs)):\n",
        "        # means and stds\n",
        "        train_mean = metrics_mean[:, train_metrics_idcs[i]]\n",
        "        val_mean = metrics_mean[:, val_metrics_idcs[i]]\n",
        "\n",
        "        train_std = metrics_std[:, train_metrics_idcs[i]]\n",
        "        val_std = metrics_std[:, val_metrics_idcs[i]]\n",
        "\n",
        "        # plot mean\n",
        "        axs[i].errorbar(epochs, train_mean, yerr=train_std, fmt='-o', capsize=3, capthick=1, label='Train')\n",
        "        axs[i].errorbar(epochs, val_mean, yerr=val_std, fmt='-o', capsize=3, capthick=1, label='Val')\n",
        "\n",
        "        axs[i].set_xlabel('Epoch')\n",
        "        axs[i].set_ylabel(titles[i])\n",
        "        axs[i].set_xlim([1, configs['num_epochs']])\n",
        "        if i == 0:\n",
        "            axs[i].set_yscale('log')\n",
        "        else:\n",
        "            axs[i].set_yticks(ticks=np.arange(0,1.1,0.1))\n",
        "        axs[i].grid(visible=True)\n",
        "        axs[i].legend()\n",
        "\n",
        "    fig.savefig(str(metrics_save_path), bbox_inches=\"tight\")\n",
        "\n",
        "\n",
        "def create_folds(img_list : list, num_folds : int) -> tuple[list, list]:\n",
        "    \"\"\"Given images and the number of folds, create training/validation sets with the most even distribution possible\"\"\"\n",
        "\n",
        "    # number of validation images to be held out in each fold\n",
        "    num_val = np.zeros(num_folds)\n",
        "\n",
        "    # randomly shuffle the image list given a numpy seed to prevent sequence bias\n",
        "    np.random.seed(203)\n",
        "    img_list = np.random.permutation(img_list)\n",
        "\n",
        "    # determine number of hold out images in each fold\n",
        "    for i in range(num_folds):\n",
        "        # start with integer quotient\n",
        "        num_val[i] = math.floor(len(img_list) / num_folds)\n",
        "        # distribute the remainder evenly among the first folds   \n",
        "        if i < (len(img_list) % num_folds):\n",
        "            num_val[i] += 1\n",
        "    \n",
        "    # convert number of hold out images to indicies\n",
        "    running_sum = np.cumsum(num_val)\n",
        "    running_sum = np.insert(running_sum, 0, 0)\n",
        "    lower_idxs = running_sum[:-1].astype(int)\n",
        "    upper_idxs = running_sum[1:].astype(int)\n",
        "\n",
        "    # save train/val sets as elements of a list\n",
        "    train_sets, val_sets = [0]*num_folds, [0]*num_folds\n",
        "    for i in range(num_folds):\n",
        "        # bounds\n",
        "        low = lower_idxs[i]\n",
        "        up = upper_idxs[i]\n",
        "\n",
        "        # fold val set\n",
        "        val_sets[i] = img_list[low:up]\n",
        "\n",
        "        # fold train set is the complement\n",
        "        train_sets[i] = np.delete(img_list, np.arange(low, up)).tolist()\n",
        "\n",
        "    return train_sets, val_sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZPv83bNLBLp"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-ma4KIcLDvR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aiden Ochoa, 4/2025, RDMAP PSU Research Group\n",
        "This module handles all training and inference operations. It has __main__\n",
        "\"\"\"\n",
        "\n",
        "class Operations:\n",
        "    \"\"\"Singleton class for training and inference\"\"\"\n",
        "\n",
        "    def __init__(self, configs: dict):\n",
        "        \"\"\"Initialize configs, dataset, and model\"\"\"\n",
        "        self.configs = configs\n",
        "        self.dataset = None\n",
        "        self.model = None\n",
        "\n",
        "    def single_loop(self):\n",
        "        \"\"\"Trains a single model using a training and validation set\"\"\"\n",
        "\n",
        "        # load dataset and model\n",
        "        print(f\"\\nCreating dataset from `{self.configs['dataset_name']}`...\\n\")\n",
        "        self.dataset = create_train_dataset(self.configs)\n",
        "        print(f\"Training images: {os_sorted(self.configs['training_set'])}\")\n",
        "        print(f\"Validation images: {os_sorted(self.configs['validation_set'])}\\n\")\n",
        "        print(f\"Loading and compiling `{self.configs['encoder_name']}-{self.configs['decoder_name']}`...\\n\")\n",
        "        self.model = load_UNet(self.configs)\n",
        "        self.model.compile(\n",
        "            optimizer = Adam(learning_rate=self.configs['learning_rate']), \n",
        "            loss = 'binary_crossentropy', \n",
        "            metrics = ['accuracy', 'Precision', 'Recall']\n",
        "        )\n",
        "\n",
        "        # define training callbacks\n",
        "        callbacks = [\n",
        "            CSVLogger(\n",
        "                str(self.configs['results_dir'] / 'metrics.csv'), \n",
        "                separator=',', \n",
        "                append=False\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                str(self.configs['results_dir'] / 'best_model.keras'), \n",
        "                verbose=1, \n",
        "                save_best_only=True, \n",
        "                save_weights_only=False\n",
        "            )\n",
        "        ]\n",
        "        if self.configs['early_stopping']:\n",
        "            callbacks.append(EarlyStopping(patience = self.configs['patience']))\n",
        "        \n",
        "        # start training\n",
        "        print(\"Training model...\\n\")\n",
        "        self.model.fit(\n",
        "            self.dataset['train_dataset'],\n",
        "            epochs = self.configs['num_epochs'],\n",
        "            steps_per_epoch = self.dataset['train_steps'],\n",
        "            validation_data = self.dataset['val_dataset'],\n",
        "            validation_steps = self.dataset['val_steps'],\n",
        "            callbacks=callbacks,\n",
        "            verbose=2\n",
        "        )\n",
        "\n",
        "        # load best model and inference train/val sets\n",
        "        print(\"\\nInferencing training and validation images with best model...\\n\")\n",
        "        self.model = load_model(str(self.configs['results_dir'] / 'best_model.keras'))\n",
        "        self.dataset = create_train_val_inference_dataset(self.configs)\n",
        "        self.inference()\n",
        "\n",
        "        # plot metrics\n",
        "        print(\"\\nPlotting metrics...\\n\")\n",
        "        plot_results(self.configs)\n",
        "\n",
        "        print(\"Cleaning up...\\n\")\n",
        "        # remove training dataset and clear memory\n",
        "        shutil.rmtree(self.configs['root_dir'] / 'dataset')\n",
        "        K.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    def crossval_loop(self):\n",
        "        \"\"\"Trains num_folds models using different non-overlapping validation sets\"\"\"\n",
        "\n",
        "        # determine all training and val set combinations given the number of folds\n",
        "        train_sets, val_sets = create_folds(self.configs['training_set'], self.configs['num_folds'])\n",
        "        \n",
        "        # save original results directory\n",
        "        top_level_results = self.configs['results_dir']\n",
        "        \n",
        "        print(f\"\\nStarting cross validation with {self.configs['num_folds']} folds...\\n\")\n",
        "\n",
        "        # loop through folds\n",
        "        for fold in range(self.configs['num_folds']):\n",
        "            print('-'*62 + ' Fold {} '.format(fold+1) + '-'*62)\n",
        "\n",
        "            # update train/val sets\n",
        "            self.configs.update({'training_set' : train_sets[fold]})\n",
        "            self.configs.update({'validation_set' : val_sets[fold]})\n",
        "\n",
        "            # create results directory for this fold\n",
        "            results_dir = self.configs['results_dir'] / ('fold_' + str(fold+1))\n",
        "            self.configs.update({'results_dir' : results_dir})\n",
        "            results_dir.mkdir()\n",
        "\n",
        "            # train fold\n",
        "            self.single_loop()\n",
        "\n",
        "            # reset results directory for next fold\n",
        "            self.configs.update({'results_dir' : top_level_results})\n",
        "\n",
        "        # plot metrics over all folds\n",
        "        print(\"Plotting cross validation results...\\n\")\n",
        "        cv_plot_results(self.configs)\n",
        "\n",
        "\n",
        "    def inference(self):\n",
        "        \"\"\"Either inferences a training-validation pair of images, or just a single set of images\"\"\"\n",
        "\n",
        "        if self.configs['operation_mode'] == 'train':\n",
        "            # process train and val images\n",
        "            train_preds = self.model.predict(self.dataset['train_dataset'], verbose=2)\n",
        "            val_preds = self.model.predict(self.dataset['val_dataset'], verbose=2)\n",
        "\n",
        "            # define and make output directories\n",
        "            train_save_dir = self.configs['results_dir'] / 'train_preds'\n",
        "            val_save_dir = self.configs['results_dir'] / 'val_preds'\n",
        "            train_save_dir.mkdir()\n",
        "            val_save_dir.mkdir()\n",
        "\n",
        "            # define pred save file paths\n",
        "            train_save_paths = [train_save_dir / (file.stem + '.png') for file in self.dataset['train_paths']]\n",
        "            val_save_paths = [val_save_dir / (file.stem + '.png') for file in self.dataset['val_paths']]\n",
        "\n",
        "            # save predictions\n",
        "            save_preds(train_preds, train_save_paths)\n",
        "            save_preds(val_preds, val_save_paths)\n",
        "\n",
        "        else:\n",
        "            # load model and dataset\n",
        "            print(\"\\nLoading data and model...\\n\")\n",
        "            self.model = load_model(str(self.configs['root_dir'] / self.configs['model_path']))\n",
        "            self.dataset = create_inference_dataset(self.configs)\n",
        "\n",
        "            # process dataset\n",
        "            print(\"Generating model predictions...\\n\")\n",
        "            preds = self.model.predict(self.dataset['dataset'], verbose=2)\n",
        "\n",
        "            # define output directories and paths\n",
        "            print(\"\\nSaving model predictions...\\n\")\n",
        "            save_dir = self.configs['results_dir'] / 'preds'\n",
        "            save_dir.mkdir()\n",
        "            save_paths = [save_dir / (file.stem + '.png') for file in self.dataset['data_paths']]\n",
        "\n",
        "            # save predictions\n",
        "            save_preds(preds, save_paths)\n",
        "\n",
        "\n",
        "    def save_model_summary(self):\n",
        "        \"\"\"Writes the model summary to a file after training and writes a file about the trainable layers\"\"\"\n",
        "\n",
        "        with open(self.configs['results_dir'] / 'model_summary.out', 'w') as f:\n",
        "            self.model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
        "\n",
        "        with open(self.configs['results_dir'] / 'trainable.out', 'w') as f:\n",
        "            f.write(f\"{'Layer':<35} {'Trainable':<20}\\n\")\n",
        "            f.write(\"=\" * 50 + \"\\n\")\n",
        "            for layer in self.model.layers:\n",
        "                f.write(f\"{layer.name:<35} {str(layer.trainable):<20}\\n\")\n",
        "\n",
        "\n",
        "def main(configs: dict):\n",
        "    \"\"\"Validates configs and instantiates operations class\"\"\"\n",
        "\n",
        "    # validate and print configs\n",
        "    configs = validate(configs)\n",
        "    print_save_configs(configs.copy())\n",
        "\n",
        "    # start operations\n",
        "    operations = Operations(configs)\n",
        "\n",
        "    # training or inference\n",
        "    if configs['operation_mode'] == 'train':\n",
        "        if configs['cross_validation']:\n",
        "            operations.crossval_loop()\n",
        "        else:\n",
        "            operations.single_loop()\n",
        "\n",
        "        if configs['model_summary']:\n",
        "            operations.save_model_summary()\n",
        "    else:\n",
        "        operations.inference()\n",
        "    \n",
        "    print(\"Done.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mQftsPxzyju"
      },
      "source": [
        "# Training/Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHt9P6vKMoKd"
      },
      "outputs": [],
      "source": [
        "# configs example\n",
        "configs = {\n",
        "    \"L2_regularization_strength\" : 1e-4,\n",
        "    \"num_epochs\" : 200,\n",
        "    \"patience\" : 25,\n",
        "    \"dataset_name\" : \"gb_512\",\n",
        "    \"batch_size\" : 2,\n",
        "    \"auto_split\" : 0.3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GauK1-HnzSg4",
        "outputId": "b45b8631-95b2-4111-a396-41a4c9397e8c"
      },
      "outputs": [],
      "source": [
        "main(configs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOoeJIRxdjFNwdggYevVgWD",
      "collapsed_sections": [
        "V3y6pXwAzpfw",
        "4mQftsPxzyju"
      ],
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
