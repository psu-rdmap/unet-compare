{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "33fb7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, PositiveInt, NonNegativeInt, PositiveFloat, NonNegativeFloat, Field, field_validator, model_validator\n",
    "from typing import Literal, List, Optional\n",
    "from pathlib import Path\n",
    "import json\n",
    "from warnings import warn\n",
    "import numpy as np\n",
    "from natsort import os_sorted\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook\n",
    "ROOT_DIR = Path.cwd().parent\n",
    "\n",
    "# Python script\n",
    "#ROOT_DIR = Path(__file__).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "cff5b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = ROOT_DIR / 'src/config_test.json'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    configs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a76728f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_files(file_set: set, master_set: set, set_type: str):\n",
    "    \"\"\"Checks if all files in a set exist in a provided directory\"\"\"\n",
    "    if not file_set <= master_set:\n",
    "        missing_fns = file_set - master_set\n",
    "        raise ValueError(f\"Could not find files with the names {missing_fns} in the dataset given in `{set_type}_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class General(BaseModel):\n",
    "    \"\"\"Most general configs that apply to all possible use cases. Only basic validation is done here.\"\"\"\n",
    "\n",
    "    operation_mode: Literal['train', 'inference'] = Field(\n",
    "        default='train',\n",
    "        description=\"General parameter that defines whether a model will be trained, or if a model will be applied for inference\"\n",
    "    )\n",
    "    dataset_name: str = Field(\n",
    "        min_length=2,\n",
    "        description=\"Dataset subdirectory prefix corresponding to unet-compare/data/<dataset_name>/\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Train(BaseModel):\n",
    "    operation_mode: str\n",
    "    dataset_name: str\n",
    "    encoder_name: Literal['UNet', 'EfficientNetB7'] = Field(\n",
    "        default='UNet',\n",
    "        description=\"Type of model architecture forming the encoder section of U-Net\"\n",
    "    )\n",
    "    decoder_name: Literal['UNet', 'UNet++'] = Field(\n",
    "        default='UNet',\n",
    "        description=\"Type of model architecture forming the decoder section of U-Net\"\n",
    "    )\n",
    "    encoder_filters: Optional[List[PositiveInt]] = Field(\n",
    "        default=None,\n",
    "        min_length=5, \n",
    "        max_length=5,\n",
    "        description=\"Number of filters to learn for each convolution. Should be `null` or ignored when `encoder_name` is `EfficientNetB7`\"\n",
    "    )\n",
    "    decoder_filters: List[PositiveInt] = Field(\n",
    "        default = [512, 256, 128, 64, 32],\n",
    "        description=\"Number of filters to learn at each resolution level. The final item is only used when `encoder_name` is `EfficientNetB7`\"\n",
    "    )\n",
    "    backbone_weights: Optional[Literal['random', 'imagenet']] = Field(\n",
    "        default=None,\n",
    "        description=\"Weights to be loaded when using a pretrained backbone. This should be ignored when `encoder_name` is `UNet`\"\n",
    "    )\n",
    "    backbone_finetuning: Optional[bool | List[NonNegativeInt]] = Field(\n",
    "        default=None,\n",
    "        max_length=7,\n",
    "        description=\"Controls the finetuning of a pretrained backbone. \" \\\n",
    "        \"This should be ignored when random weights are used like when `encoder_name` is `UNet` and when `backbone_weights` is `random`.\" \\\n",
    "        \"The entire backbone can be unfrozen (bool), or selected blocks (array of ints)\"\n",
    "    )\n",
    "    learning_rate: PositiveFloat = Field(\n",
    "        default=1e-4,\n",
    "        lt=1.0,\n",
    "        description=\"Learning rate for the Adam optimizer. Should be between 0 and 1\"\n",
    "    )\n",
    "    L2_regularization_strength: NonNegativeFloat = Field(\n",
    "        default=0.0,\n",
    "        lt=1.0,\n",
    "        description=\"Strength of L2 regularization used during training. Should be between 0 and 1, with 0 meaning no L2 regularization is used\"\n",
    "    )\n",
    "    batch_size: PositiveInt = Field(\n",
    "        default=4,\n",
    "        description=\"Number of image-annotation pairs to use in a single batch. Each batch represents one weight vector update\"\n",
    "    )\n",
    "    num_epochs: PositiveInt = Field(\n",
    "        default=50,\n",
    "        description=\"Number of epochs to train for. Each epoch is one pass through the entire dataset\"\n",
    "    )\n",
    "    augment: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Augment the training subset eightfold by flipping and rotating by 90 deg intervals\"\n",
    "    )\n",
    "    save_model: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Save the model to the results file\"\n",
    "    )\n",
    "    standardize: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Standardize the dataset using image statistics from the train subset\"\n",
    "    )\n",
    "    cross_validation: bool = Field(\n",
    "        default=False,\n",
    "        description=\"Performs a k-fold cross validation study where many models are trained using different non-overlapping validation sets\"\n",
    "    )\n",
    "    num_folds: Optional[PositiveInt] = Field(\n",
    "        default=None,\n",
    "        gt=1,\n",
    "        description=\"Number of models to train for cross validation. Should be ignored when `cross_validation` is `false`\"\n",
    "    )\n",
    "    early_stopping: Optional[bool] = Field(\n",
    "        default=None,\n",
    "        description=\"Stop the training if the validation loss does not improve after a given number of epochs provided by `patience`. \" \\\n",
    "        \"Does not apply when `cross_validation` is `true`\"\n",
    "    )\n",
    "    patience: Optional[PositiveInt] = Field(\n",
    "        default=None,\n",
    "        description=\"Number of epochs before training is stopped automatically. Only applies when `early_stopping` is `true`\"\n",
    "    )\n",
    "    training_set: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"Array of image filenames to be used for the training set. Reference the logical tree to see how it may be defined\"\n",
    "    )\n",
    "    validation_set: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"Array of image filenames to be used for the validation set. Reference the logical tree to see how it may be defined\"\n",
    "    )\n",
    "    auto_split: Optional[PositiveFloat] = Field(\n",
    "        default=None, \n",
    "        lt=1,\n",
    "        description=\"Validation hold out percentage used when automatically splitting the dataset. Reference the logical tree to see how it may be defined\"\n",
    "    )\n",
    "    model_summary: bool = Field(\n",
    "        default=True,\n",
    "        description=\"Print out the model summary from Keras to a log file in the results directory\"\n",
    "    )\n",
    "    results_dir: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"Path for results/output directory relative to /path/to/unet-compare/. Give it `null` or ignore it to use default naming scheme\"\n",
    "    )\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def pretrained_backbone(self) -> 'Train':\n",
    "        \"\"\"Validation specific to pretrained backbones (EfficientNet)\"\"\"\n",
    "        \n",
    "        # set the encoder filters if not supplied and not using EfficientNet encoder\n",
    "        if self.encoder_name == 'UNet' and self.encoder_filters is None:\n",
    "            self.encoder_filters=[64, 128, 256, 512, 1024]\n",
    "        \n",
    "        # backbone weights should only be specified when using EfficientNet\n",
    "        if self.encoder_name == 'UNet' and self.backbone_weights is not None:\n",
    "            self.backbone_weights == None\n",
    "            warn(\"`backbone_weights` should be `null` or ignored when `encoder_name` is `UNet`\")\n",
    "        elif self.encoder_name == 'EfficientNetB7' and self.backbone_weights is None: \n",
    "            raise ValueError(f\"`backbone_weights` should be `random` or `imagenet` when `encoder_name` is `EfficientNetB7`\")\n",
    "        \n",
    "        # backbone finetuning should be none when using random weights (UNet or random EfficientNet)\n",
    "        if self.encoder_name == 'UNet' and self.backbone_finetuning is not None:\n",
    "            self.backbone_finetuning = None\n",
    "            warn(\"`backbone_finetuning` should be `null` or ignored when `encoder_name` is `UNet`\")\n",
    "        elif self.encoder_name == 'EfficientNetB7' and self.backbone_weights == 'random':\n",
    "            self.backbone_finetuning = None\n",
    "            warn(\"`backbone_finetuning` should be `null` when `backbone_weights` is `random`\")\n",
    "\n",
    "        # block level unfreezing should be an array of block ints (0,1,2,...,7) \n",
    "        if self.encoder_name == 'EfficientNetB7' and type(self.backbone_weights) == list:\n",
    "            assert len(set(self.backbone_weights)) == len(self.backbone_weights), \"All block indices must be unique in `backbone_finetuning`\"\n",
    "            assert np.all(np.array(self.backbone_weights) < 8), \"Block indices must be from 0 to 7 in `backbone_finetuning`\"      \n",
    "\n",
    "        return self\n",
    "\n",
    "    @field_validator('dataset_name', mode='after')\n",
    "    @classmethod\n",
    "    def check_dataset(cls, dataset_name : str) -> str:\n",
    "        \"\"\"Checks various aspects about the dataset name provided\"\"\"\n",
    "        \n",
    "        # check if the dataset directory exists\n",
    "        abs_path = ROOT_DIR / 'data' / dataset_name\n",
    "        if not abs_path.exists():\n",
    "            raise ValueError(f\"Dataset can not be found at `{abs_path}`\")\n",
    "\n",
    "        # check if the dataset has the proper subdirectories\n",
    "        img_subdir = abs_path / 'images'\n",
    "        ann_subdir = abs_path / 'annotations'\n",
    "        if not img_subdir.exists():\n",
    "            raise ValueError(f\"Dataset is missing the `images/` subdirectory\")\n",
    "        elif not ann_subdir.exists():\n",
    "            raise ValueError(f\"Dataset is missing the `annotations/` subdirectory\")\n",
    "        \n",
    "        # check if the dataset subdirectories have no child directories themselves (only files)\n",
    "        img_childdirs = [path.is_dir() for path in img_subdir.iterdir()]\n",
    "        ann_childdirs = [path.is_dir() for path in ann_subdir.iterdir()]\n",
    "        if any(img_childdirs):\n",
    "            raise ValueError(\"`images/` subdirectory should contain files, not directories\")\n",
    "        elif any(ann_childdirs):\n",
    "            raise ValueError(\"`annotations/` subdirectory should contain files, not directories\")\n",
    "            \n",
    "        # check if the dataset subdirectories have at least 2 files\n",
    "        img_files = list(img_subdir.iterdir())\n",
    "        ann_files = list(ann_subdir.iterdir())\n",
    "        num_imgs = len(img_files)\n",
    "        num_anns = len(ann_files)\n",
    "        if num_imgs != num_anns:\n",
    "            raise ValueError(\n",
    "                f\"There must be the same number of images and annotations. Got {num_imgs} image files and {num_anns} annotation files\")\n",
    "        elif num_imgs < 2:\n",
    "            raise ValueError(\"There must be at least 2 image/annotation file pairs\")\n",
    "        elif len(set(img_files)) != num_imgs: # set removes duplicates\n",
    "            raise ValueError(\"Every image/annotation filename must be unique\")\n",
    "        \n",
    "        # check if image/annotations have mixed file types \n",
    "        img_ext = {file.suffix for file in img_files}\n",
    "        ann_ext = {file.suffix for file in ann_files}\n",
    "        if len(img_ext) != 1:\n",
    "            raise ValueError(f'Images must have the same file type. Got types {img_ext}')\n",
    "        elif len(ann_ext) != 1:\n",
    "            raise ValueError(f'Annotations must have the same file type. Got types {ann_ext}')\n",
    "        \n",
    "        # make sure image/annotations are are JPEGs or PNGs\n",
    "        img_ext = next(iter(img_ext)) # gets element of singleton set\n",
    "        ann_ext = next(iter(ann_ext))\n",
    "        allowed_file_types = ['.jpg', '.jpeg', '.png']\n",
    "        if img_ext not in allowed_file_types:\n",
    "            raise ValueError(f'Expected image filetype to be one of {allowed_file_types}, got {img_ext}')\n",
    "        if ann_ext not in allowed_file_types:\n",
    "            raise ValueError(f'Expected annotation filetype to be one of {allowed_file_types}, got {ann_ext}')\n",
    "               \n",
    "        # check if every image has a corresponding annotation (by name)\n",
    "        img_stems = {file.stem for file in img_files}\n",
    "        ann_stems = {file.stem for file in ann_files}\n",
    "        if img_stems != ann_stems:\n",
    "            unpaired_stems = img_stems ^ ann_stems # get disjointed elements (unique to each set)\n",
    "            raise ValueError(f'Found unpaired image or annotation files with filenames {unpaired_stems}')\n",
    "\n",
    "        return dataset_name\n",
    "    \n",
    "    @field_validator('batch_size', mode='after')\n",
    "    @classmethod\n",
    "    def batch_size_warning(cls, batch_size : int):\n",
    "        \"\"\"Warn the user if batch size is not a power of 2\"\"\"\n",
    "        if np.log2(batch_size) % 1 != 0.0:\n",
    "            warn(\"`batch_size` is not a power of two. Efficiency may be reduced\")\n",
    "    \n",
    "    @model_validator(mode='after')\n",
    "    def check_early_stopping(self) -> 'Train':\n",
    "        \"\"\"Checks early_stopping and patience fields and validate when doing cross validation\"\"\"\n",
    "        \n",
    "        # patience should only be provided when using early stopping and should be less than the number of epochs\n",
    "        if self.early_stopping == False and self.patience is not None:\n",
    "            self.patience = None\n",
    "            warn(f\"`patience` should be `null` if `early_stopping` is `false`. Changed `patience` to `null`\")\n",
    "        elif self.early_stopping == True:\n",
    "            assert self.patience < self.num_epochs, \"`patience` can not be greater than `num_epochs`\"\n",
    "\n",
    "        # early_stopping should be null when doing cross validation\n",
    "        if self.cross_validation is True:\n",
    "            if self.early_stopping is not None or self.patience is not None:\n",
    "                self.early_stopping = None\n",
    "                self.patience = None\n",
    "                warn(\"`early_stopping` and `patience` should be `null` or ignored when `cross_validation` is `true`\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    @model_validator(mode='after')\n",
    "    def check_train_val(self) -> 'Train':\n",
    "        \"\"\"Checks many aspects of the train-val splitting when training single models and doing cross validation. It applies the logical tree from the docs\"\"\"\n",
    "        \n",
    "        # dataset has already been validated since field_validators run first\n",
    "        data_dir = ROOT_DIR / 'data' / self.dataset_name / 'images'\n",
    "        img_stems = [file.stem for file in data_dir.iterdir()]\n",
    "        img_stems_set = set(img_stems)\n",
    "\n",
    "        # basic cross validation check \n",
    "        if self.cross_validation == True:\n",
    "            # val sets will be determined algorithmically\n",
    "            assert self.validation_set is None, \"`validation_set` should be `null` or ignored when `cross_validation` is `true`\"\n",
    "\n",
    "        # logical tree with train at the top (see docs)\n",
    "\n",
    "        # ------------ FORK 1: train set provided or not ------------ #\n",
    "        if self.training_set is not None:\n",
    "            # make sure all train files exist\n",
    "            training_set = set(self.training_set)\n",
    "            check_files(training_set, img_stems_set, 'train')\n",
    "\n",
    "            # ------------ FORK 2: cross validation is true or not ------------ # \n",
    "            if self.cross_validation == True:\n",
    "                # check number of folds\n",
    "                assert self.num_folds < len(self.training_set), \"`num_folds` can not be greater than the number of training images used for cross validation\"\n",
    "\n",
    "            else:\n",
    "                # ------------ FORK 3: val set is provided or not ------------ # \n",
    "                if self.validation_set is not None:\n",
    "                    # check overlap between train and val sets and make sure val set files exist\n",
    "                    validation_set = set(self.validation_set)\n",
    "                    train_val_overlap = training_set & validation_set\n",
    "                    assert len(train_val_overlap) == 0, f\"Files with the names {train_val_overlap} were found in both `training_set` and `validation_set`\"\n",
    "                    check_files(validation_set, img_stems_set, 'validation')\n",
    "                else:\n",
    "\n",
    "                    # ------------ FORK 4: auto split is provided or not ------------ # \n",
    "                    if self.auto_split:\n",
    "                        # make sure auto_split is not too large where no train set is created\n",
    "                        assert self.auto_split < (1 / len(self.training_set)), f\"auto_split validation hold-out percentage must be less than {1/len(self.training_set)} for the `training_set` provided\"\n",
    "                    else:\n",
    "                        # val set is the complement of train; make sure train does not have all available images\n",
    "                        assert not training_set < img_stems_set, \"`training_set` can not have all images in the dataset. Some must be left over for `validation_set`\"\n",
    "                        # generate val set (sort them naturally)\n",
    "                        self.validation_set = os_sorted(list(img_stems_set - training_set))\n",
    "\n",
    "        # ------------ FORK 1: back to the top (train set not provided) ------------ # \n",
    "        else:\n",
    "\n",
    "            # ------------ FORK 2: cross validation is true or not ------------ # \n",
    "            if self.cross_validation == True:\n",
    "                self.training_set = img_stems\n",
    "            else:\n",
    "\n",
    "                # ------------ FORK 3: val_set is provided or not ------------ #\n",
    "                if self.validation_set is not None:\n",
    "                    # train set is the complement to val set\n",
    "                    validation_set = set(self.validation_set)\n",
    "                    assert not validation_set < img_stems_set, \"`validation_set` can not have all images in the dataset. Some must be left over for `training_set`\"\n",
    "                    # generate train set (sort them naturally)\n",
    "                    self.training_set = os_sorted(list(img_stems_set - validation_set))\n",
    "                else:\n",
    "                    # define auto_split if it is not provided, or just check its value\n",
    "                    if not self.auto_split:\n",
    "                        self.auto_split = 0.4\n",
    "                        warn(\"`auto_split` validation hold-out percentage not provided even though `training_set` and `validation_set` are `null` and `cross_validation` is `false`. Defaulting to 40%\")\n",
    "                    else:      \n",
    "                        assert self.auto_split < 1 / len(img_stems_set), f\"`auto_split` validation hold-out percentage must be less {1/len(img_stems_set)}\"\n",
    "\n",
    "        return self\n",
    "    \n",
    "    @model_validator(mode='after')\n",
    "    def generate_results_dir(self) -> 'Train':\n",
    "        \"\"\"Create the results directory following a naming scheme if one is not provided\"\"\"\n",
    "\n",
    "        if self.results_dir is None:\n",
    "            now = datetime.now()\n",
    "            results_dir = 'results_' + self.dataset_name + '_' + self.operation_mode + '_' + self.encoder_name + '_' + self.decoder_name \n",
    "            if self.cross_validation:\n",
    "                results_dir += '_crossval'\n",
    "            results_dir += now.strftime('_(%Y-%m-%d)_(%H-%M-%S)')\n",
    "            results_dir = ROOT_DIR / results_dir\n",
    "            self.results_dir = results_dir\n",
    "        \n",
    "        return self\n",
    "\n",
    "class Inference(BaseModel):\n",
    "    dataset_name: str\n",
    "    model_path: str = Field(\n",
    "        description=\"Path relative to /path/to/unet-compare/ to an existing model to be used for inference\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('model_path', mode='after')\n",
    "    @classmethod\n",
    "    def check_model(cls, model_path) -> 'General':\n",
    "        # check if the model file exists\n",
    "        abs_path = ROOT_DIR / model_path\n",
    "        if not abs_path.exists():\n",
    "            raise ValueError(f'No model file exists at `{abs_path}`')\n",
    "\n",
    "        # check if it is a .keras model file\n",
    "        ext = abs_path.name.split('.', 1)[-1]\n",
    "        if ext == 'weights.h5':\n",
    "            raise ValueError('Expected a .keras model file, got a weights.h5 file instead')\n",
    "        elif ext == 'keras':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f'Expected a .keras file, got a .{ext} file')\n",
    "        \n",
    "        return model_path   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c8525b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1078/1729021996.py:296: UserWarning: `auto_split` validation hold-out percentage not provided even though `training_set` and `validation_set` are `null` and `cross_validation` is `false`. Defaulting to 40%\n",
      "  warn(\"`auto_split` validation hold-out percentage not provided even though `training_set` and `validation_set` are `null` and `cross_validation` is `false`. Defaulting to 40%\")\n"
     ]
    }
   ],
   "source": [
    "general = General.model_validate(configs)\n",
    "\n",
    "# select the validator specific to the operation mode\n",
    "if general.operation_mode == 'train':\n",
    "    train = Train.model_validate(configs)\n",
    "else:\n",
    "    inference = Inference.model_validate(configs)\n",
    "\n",
    "# save configs into results dir for reference\n",
    "with open(ROOT_DIR / 'configs.json', 'w') as con:\n",
    "    json.dump(train.model_dump(), con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7c819750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9_2'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = ROOT_DIR / 'data' / 'gb_512' / 'images'\n",
    "\n",
    "train_fns = ['1_1', '5_3', '9_2', '24_2']\n",
    "val_fns = ['1_2', '5_2', '9_2']\n",
    "\n",
    "set(train_fns) & set(val_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d270abb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_(2025-04-15)_(14-30-45)'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"1\"\n",
    "a += 'b'\n",
    "datetime.now().strftime('_(%Y-%m-%d)_(%H-%M-%S)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
